"""
Stock Data Processor Module
Centralized handling for fetching and preprocessing stock data for various models.
"""

import yfinance as yf
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from datetime import datetime, timedelta

class StockDataProcessor:
    """
    Handles data fetching, feature engineering, and formatting for different model types.
    """
    
    def __init__(self, ticker: str = "AAPL"):
        self.ticker = ticker
        self.data: pd.DataFrame = None
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.target_scaler = MinMaxScaler(feature_range=(0, 1)) # Scaler just for the target value
        
    def fetch_data(self, years: int = 5) -> pd.DataFrame:
        """Fetch historical stock data using yfinance."""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=years * 365)
        
        stock = yf.Ticker(self.ticker)
        df = stock.history(start=start_date, end=end_date)
        
        if df.empty:
            raise ValueError(f"No data found for ticker {self.ticker}")
        
        self.data = df
        return df

    def add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add Technical Indicators (SMA, RSI, MACD) to the dataframe."""
        df = df.copy()
        
        # Simple Moving Average (SMA)
        df['SMA_20'] = df['Close'].rolling(window=20).mean()
        df['SMA_50'] = df['Close'].rolling(window=50).mean()
        
        # Relative Strength Index (RSI)
        delta = df['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['RSI'] = 100 - (100 / (1 + rs))
        
        # MACD (Moving Average Convergence Divergence)
        exp1 = df['Close'].ewm(span=12, adjust=False).mean()
        exp2 = df['Close'].ewm(span=26, adjust=False).mean()
        df['MACD'] = exp1 - exp2
        df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()
        
        # Fill NaN values generated by indicators
        df.fillna(method='bfill', inplace=True)
        df.fillna(method='ffill', inplace=True) # Fallback
        
        return df

    def prepare_for_lstm(self, df: pd.DataFrame, sequence_length: int = 60, target_col: str = 'Close'):
        """
        Prepare data specifically for LSTM input (3D array) with Multiple Features.
        Returns: X_train, y_train, X_test, y_test, scaler, df
        """
        # Enrichment step
        data = self.add_technical_indicators(df)
        
        # Define features to use
        feature_cols = ['Close', 'SMA_20', 'SMA_50', 'RSI', 'MACD', 'Signal_Line']
        
        # Save feature columns for later use
        self.feature_cols = feature_cols
        
        # SPLIT DATA FIRST (80/20) to prevent leakage
        split_idx = int(len(data) * 0.8)
        train_df = data.iloc[:split_idx]
        test_df = data.iloc[split_idx:]
        
        # FIT scalers ONLY on training data
        self.scaler.fit(train_df[feature_cols].values)
        target_values = train_df[target_col].values.reshape(-1, 1)
        self.target_scaler.fit(target_values)
        
        # Transform both sets using the Training scaler
        train_scaled = self.scaler.transform(train_df[feature_cols].values)
        test_scaled = self.scaler.transform(test_df[feature_cols].values)
        
        # Helper to create sequences
        def create_sequences(dataset):
            X, y = [], []
            target_idx = feature_cols.index(target_col)
            for i in range(sequence_length, len(dataset)):
                X.append(dataset[i - sequence_length:i, :]) # All features
                y.append(dataset[i, target_idx]) # Only target
            return np.array(X), np.array(y)
            
        # Create sequences for both sets
        X_train, y_train = create_sequences(train_scaled)
        
        # For Test set, we need to concatenate the last 'sequence_length' from train 
        # to ensure we don't lose the first few test points
        # But for strict separation, standard practice is just processing test_scaled
        X_test, y_test = create_sequences(test_scaled)
        
        # Reshape is automatic since X is [samples, seq_len, features]
        
        return X_train, y_train, X_test, y_test, self.target_scaler, data

    def get_latest_sequence(self, df: pd.DataFrame, sequence_length: int = 60, target_col: str = 'Close'):
        """Get the latest sequence for prediction with all features."""
        # Ensure indicators exist
        if 'RSI' not in df.columns:
            df = self.add_technical_indicators(df)
            
        feature_cols = ['Close', 'SMA_20', 'SMA_50', 'RSI', 'MACD', 'Signal_Line']
        dataset = df[feature_cols].values[-sequence_length:]
        
        scaled = self.scaler.transform(dataset)
        return np.reshape(scaled, (1, sequence_length, len(feature_cols)))

# Helper functions for backward compatibility or simple usage
def fetch_stock_data(ticker="AAPL", years=5):
    processor = StockDataProcessor(ticker)
    return processor.fetch_data(years)

def prepare_data_for_lstm(df, sequence_length=60):
    processor = StockDataProcessor()
    return processor.prepare_for_lstm(df, sequence_length)

def get_latest_sequence(df, scaler, sequence_length=60):
    # Note: This helper requires the passed scaler, ignoring the processor's internal one
    # to maintain compatibility with existing app logic that passes scaler around
    processor = StockDataProcessor()
    processor.scaler = scaler 
    return processor.get_latest_sequence(df, sequence_length)
